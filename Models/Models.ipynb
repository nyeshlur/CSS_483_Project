{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import scipy.io\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from interpret import show\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dt(data, label, seed=None):\n",
    "    random.seed(seed)\n",
    "    ones = []\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 1:\n",
    "            ones.append(i)\n",
    "    zeros = []\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 0:\n",
    "            zeros.append(i)\n",
    "    zeros = random.sample(zeros, len(ones))\n",
    "    indices = zeros + ones\n",
    "    X = data[indices]\n",
    "    y = label[indices]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_reversed(data, label, seed=None):\n",
    "    random.seed(seed)\n",
    "    zeros = []\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 0:\n",
    "            zeros.append(i)\n",
    "    ones = []\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 1:\n",
    "            ones.append(i)\n",
    "    ones = random.sample(ones, len(zeros))\n",
    "    indices = zeros + ones\n",
    "    X = data[indices]\n",
    "    y = label[indices]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_kfold(best_clf, x, y, org_dt, org_lb):\n",
    "    cros_res = cross_val_score(best_clf, x, y, cv=5, scoring='accuracy')\n",
    "    print(\"cross_res accuracy\", np.mean(cros_res))\n",
    "    cros_res = cross_val_score(best_clf, org_dt, org_lb, cv=5, scoring='accuracy')\n",
    "    print(\"cross_res accuracy\", np.mean(cros_res))\n",
    "\n",
    "    cros_res = cross_val_score(best_clf, x, y, cv=5, scoring='precision')\n",
    "    print(\"cross_res precision\", np.mean(cros_res))\n",
    "    cros_res = cross_val_score(best_clf, org_dt, org_lb, cv=5, scoring='precision')\n",
    "    print(\"cross_res precision\", np.mean(cros_res))\n",
    "\n",
    "    cros_res = cross_val_score(best_clf, x, y, cv=5, scoring='recall')\n",
    "    print(\"cross_res recall\", np.mean(cros_res))\n",
    "    cros_res = cross_val_score(best_clf, org_dt, org_lb, cv=5, scoring='recall')\n",
    "    print(\"cross_res recall\", np.mean(cros_res))\n",
    "\n",
    "    cros_res = cross_val_score(best_clf, x, y, cv=5, scoring='f1')\n",
    "    print(\"cross_res f1\", np.mean(cros_res))\n",
    "    cros_res = cross_val_score(best_clf, org_dt, org_lb, cv=5, scoring='f1')\n",
    "    print(\"cross_res f1\", np.mean(cros_res))\n",
    "\n",
    "    cros_res = cross_val_score(best_clf, x, y, cv=5, scoring='roc_auc')\n",
    "    print(\"cross_res auc\", np.mean(cros_res))\n",
    "    cros_res = cross_val_score(best_clf, org_dt, org_lb, cv=5, scoring='roc_auc')\n",
    "    print(\"cross_res auc\", np.mean(cros_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('data.mat')\n",
    "org_dat = mat['OriginalData']\n",
    "stand_dat = mat['Scaled_Standardization']\n",
    "minmax_dat = mat['Scaled_Min_Max']\n",
    "label = mat['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sc = 0\n",
    "best_x = []\n",
    "best_y = []\n",
    "best_es = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    random.seed(i)\n",
    "    X, y = balance_dt(minmax_dat, label, seed=i)\n",
    "\n",
    "    parameters = {'kernel': ['linear', 'poly', 'rbf'], 'random_state': [i], 'C': [1, 2, 3, 4, 5]}\n",
    "\n",
    "    clf = GridSearchCV(SVC(probability=True), parameters, n_jobs=-1, cv=5, verbose=1, scoring='f1')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    if clf.best_score_ > best_sc:\n",
    "        best_sc = clf.best_score_\n",
    "        best_es = clf.best_estimator_\n",
    "        best_x = X\n",
    "        best_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------Results--------------------\")\n",
    "print(\"Best score: \", best_sc)\n",
    "print(best_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_with_kfold(best_es, best_x, best_y, minmax_dat, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of SVM Results Using LIME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import LimeTabular\n",
    "\n",
    "seed = 1\n",
    "X, y = balance_dt(minmax_dat, label, seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "pca = PCA()\n",
    "svc = SVC(gamma='auto', probability=True)\n",
    "\n",
    "blackbox_model = Pipeline([('pca', pca), ('svc', svc)])\n",
    "blackbox_model.fit(X_train, y_train)\n",
    "\n",
    "lime = LimeTabular(predict_fn=blackbox_model.predict_proba, data=X_train)\n",
    "lime_local = lime.explain_local(X_test[:5], y_test[:5])\n",
    "\n",
    "show(lime_local)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import LimeTabular\n",
    "\n",
    "seed = 1\n",
    "X = minmax_dat\n",
    "y = label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "pca = PCA()\n",
    "svc = SVC(gamma='auto', probability=True)\n",
    "\n",
    "blackbox_model = Pipeline([('pca', pca), ('svc', svc)])\n",
    "blackbox_model.fit(X_train, y_train)\n",
    "\n",
    "lime = LimeTabular(predict_fn=blackbox_model.predict_proba, data=X_train)\n",
    "lime_local = lime.explain_local(X_test[:5], y_test[:5])\n",
    "\n",
    "show(lime_local)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sc = 0\n",
    "best_x = []\n",
    "best_y = []\n",
    "best_es = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    random.seed(i)\n",
    "    X, y = balance_dt(minmax_dat, label, seed=i)\n",
    "\n",
    "    parameters = {'activation': ['relu'], 'solver': ['sgd'],\n",
    "                  'learning_rate': ['constant'],\n",
    "                  'hidden_layer_sizes': (90,),\n",
    "                  'max_iter': [200, 500, 1000], 'random_state': [i]}\n",
    "\n",
    "    clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1, cv=5, verbose=1, scoring='f1')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    if clf.best_score_ > best_sc:\n",
    "        best_sc = clf.best_score_\n",
    "        best_es = clf.best_estimator_\n",
    "        best_x = X\n",
    "        best_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------Results--------------------\")\n",
    "print(\"Best score: \", best_sc)\n",
    "print(best_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_with_kfold(best_es, best_x, best_y, minmax_dat, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of CNN Results Using Kernel SHAP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import ShapKernel\n",
    "seed = 1\n",
    "X, y = balance_dt(minmax_dat, label, seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "pca = PCA()\n",
    "cnn = MLPClassifier(random_state = seed, max_iter=200)\n",
    "\n",
    "blackbox_model = Pipeline([('pca', pca), ('cnn', cnn)])\n",
    "blackbox_model.fit(X_train, y_train)\n",
    "\n",
    "shap = ShapKernel(predict_fn=blackbox_model.predict_proba, data=X_train)\n",
    "shap_local = shap.explain_local(X_test[:5], y_test[:5])\n",
    "\n",
    "show(shap_local)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import ShapKernel\n",
    "seed = 1\n",
    "X = minmax_dat\n",
    "y = label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "pca = PCA()\n",
    "cnn = MLPClassifier(random_state = seed, max_iter=200)\n",
    "\n",
    "blackbox_model = Pipeline([('pca', pca), ('cnn', cnn)])\n",
    "blackbox_model.fit(X_train, y_train)\n",
    "\n",
    "shap = ShapKernel(predict_fn=blackbox_model.predict_proba, data=X_train)\n",
    "shap_local = shap.explain_local(X_test[:5], y_test[:5])\n",
    "\n",
    "show(shap_local)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sc = 0\n",
    "best_x = []\n",
    "best_y = []\n",
    "best_es = None\n",
    "initial_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    random.seed(i)\n",
    "    X, y = balance_dt(minmax_dat, label, seed=i)\n",
    "\n",
    "    parameters = {'criterion': ['gini', 'entropy'],\n",
    "                  'min_samples_leaf': np.arange(1, 10),\n",
    "                  'min_samples_split': np.arange(2, 10), 'random_state': [i], }\n",
    "\n",
    "    clf = GridSearchCV(DecisionTreeClassifier(), parameters, n_jobs=-1, cv=5, verbose=1, scoring='f1')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    if clf.best_score_ > best_sc:\n",
    "        best_sc = clf.best_score_\n",
    "        best_es = clf.best_estimator_\n",
    "        best_x = X\n",
    "        best_y = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------Results--------------------\")\n",
    "print(\"Best score: \", best_sc)\n",
    "print(best_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_with_kfold(best_es, best_x, best_y, minmax_dat, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting Decision Tree Using Glassbox Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ClassificationTree\n",
    "\n",
    "seed = 1\n",
    "X, y = balance_dt(minmax_dat, label, seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "dt = ClassificationTree(random_state=seed)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "dt_global = dt.explain_global()\n",
    "\n",
    "show(dt_global)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ClassificationTree\n",
    "\n",
    "seed = 1\n",
    "X = minmax_dat\n",
    "y = label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "dt = ClassificationTree(random_state=seed)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "dt_global = dt.explain_global()\n",
    "\n",
    "show(dt_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sc = 0\n",
    "best_x = []\n",
    "best_y = []\n",
    "best_es = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    random.seed(i)\n",
    "    X, y = balance_dt(minmax_dat, label, seed=i)\n",
    "\n",
    "    parameters = {'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "                  'random_state': [i], 'max_iter': [100, 300, 500, 1000]}\n",
    "\n",
    "    clf = GridSearchCV(LogisticRegression(), parameters, n_jobs=-1, cv=10, verbose=1, scoring='recall')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    if clf.best_score_ > best_sc:\n",
    "        best_sc = clf.best_score_\n",
    "        best_es = clf.best_estimator_\n",
    "        best_x = X\n",
    "        best_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------Results--------------------\")\n",
    "print(\"Best score: \", best_sc)\n",
    "print(best_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_with_kfold(best_es, best_x, best_y, minmax_dat, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting Logistic Regression Using Glassbox Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import LogisticRegression\n",
    "\n",
    "seed = 1\n",
    "X, y = balance_dt(minmax_dat, label, seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "lr = LogisticRegression(random_state = seed)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr_global = lr.explain_global()\n",
    "show(lr_global)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import LogisticRegression\n",
    "\n",
    "seed = 1\n",
    "X = minmax_dat\n",
    "y = label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "lr = LogisticRegression(random_state = seed)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr_global = lr.explain_global()\n",
    "show(lr_global)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainable Boosting Machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "seed = 1\n",
    "X, y = balance_dt(minmax_dat, label, seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "ebm = ExplainableBoostingClassifier(random_state=seed)\n",
    "ebm.fit(X_train, y_train)\n",
    "\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "seed = 1\n",
    "X = minmax_dat\n",
    "y = label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "ebm = ExplainableBoostingClassifier(random_state=seed)\n",
    "ebm.fit(X_train, y_train)\n",
    "\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sc = 0\n",
    "best_x = []\n",
    "best_y = []\n",
    "best_es = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    random.seed(i)\n",
    "    X, y = balance_dt(minmax_dat, label, seed=i)\n",
    "\n",
    "    parameters = {'n_neighbors': np.arange(1, 11), 'p': [1, 2]}\n",
    "\n",
    "    clf = GridSearchCV(KNeighborsClassifier(), parameters, n_jobs=-1, cv=5, verbose=1, scoring='recall')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    if clf.best_score_ > best_sc:\n",
    "        best_sc = clf.best_score_\n",
    "        best_es = clf.best_estimator_\n",
    "        best_x = X\n",
    "        best_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------Results--------------------\")\n",
    "print(\"Best score: \", best_sc)\n",
    "print(best_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_with_kfold(best_es, best_x, best_y, minmax_dat, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting KNN Using Partial Dependence Plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import PartialDependence\n",
    "seed = 1\n",
    "X, y = balance_dt(minmax_dat, label, seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "pca = PCA()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "blackbox_model = Pipeline([('pca', pca), ('knn', knn)])\n",
    "blackbox_model.fit(X_train, y_train)\n",
    "\n",
    "pdp = PartialDependence(predict_fn=blackbox_model.predict_proba, data=X_train)\n",
    "pdp_global = pdp.explain_global()\n",
    "\n",
    "show(pdp_global)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import PartialDependence\n",
    "seed = 1\n",
    "X = minmax_dat\n",
    "y = label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = seed)\n",
    "\n",
    "pca = PCA()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "blackbox_model = Pipeline([('pca', pca), ('knn', knn)])\n",
    "blackbox_model.fit(X_train, y_train)\n",
    "\n",
    "pdp = PartialDependence(predict_fn=blackbox_model.predict_proba, data=X_train)\n",
    "pdp_global = pdp.explain_global()\n",
    "\n",
    "show(pdp_global)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
